
---
output: 
  html_document:
  pdf_document: default
  word_document: default
title: "Assignment 14: Twitters Mining with rtweet"
---

***How to do it?***: 

- Open the Rmarkdown file of this assignment ([link](fa2021_assignment14.Rmd)) in Rstudio. 

- Right under each **question**, insert  a code chunk (you can use the hotkey `Ctrl + Alt + I` to add a code chunk) and code the solution for the question. 

- `Knit` the rmarkdown file (hotkey: `Ctrl + Alt + K`) to export an html.  

-  Publish the html file to your Githiub Page. 

***Submission***: Submit the link on Github of the assignment to Canvas

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE)
```


[Sample Codes](rtweet2.html)

-------

1. Pick a keyword or hashtag. Download the data associated with the keyword/hashtag. Plot at least 10 plots to visualize the data associated with the keyword/hashtag. All plots should have titles and captions.
```{r}
library(rtweet) 
library(tidytext)
library(ggpubr) 
library(tidyverse) 
library(knitr)
library(lubridate)
```


```{r}
#keyword_search = '#bollywood'

#df <- search_tweets(q = keyword_search, 
#                        n = 18000,
#                        include_rts = FALSE,
#                        `-filter` = "replies",
#                        lang = "en") %>% 
#  mutate(created_at = ymd_hms(format(created_at, tz = "US/Eastern")))

#write_csv(df, 'twitter_bollywood.csv')
```

```{r}
df <- read_csv('twitter_bollywood.csv')
```

```{r}
df %>% 
  unnest_tokens(output = hashtag, input = text, token = "tweets") %>%
  filter(!hashtag %in% c("#bollywood",'#movie','#india','#indian'), str_detect(hashtag, "^#")) %>% 
  count(hashtag, sort = TRUE) %>%
  head(10)
```
```{r}
df %>% 
  unnest_tokens(output = hashtag, input = text, token = "tweets") %>%
  filter(!hashtag %in% c("#bollywood",'#movie','#india','#indian'), str_detect(hashtag, "^#")) %>%
  count(hashtag, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(hashtag, n)))+
  geom_col()+
  labs(title = "Popular Hashtags", x = 'Frequency', y = '', caption = "The most popular hashtag right now is #katrinakaif, who is a very famous Bollywood actor. She got married recently which is why she is trending under Bollywood related hashtags.")
```

```{r}
library(RColorBrewer)

pal <- brewer.pal(8,"Dark2")
library(wordcloud) 

df %>% 
  unnest_tokens(output = hashtag, input = text, token = "tweets") %>%
  filter(!hashtag %in% c("#bollywood",'#movie','#india','#indian'), str_detect(hashtag, "^#")) %>%
  count(hashtag, sort = TRUE) %>%
  mutate(hashtag = str_remove(hashtag, '#')) %>% 
  filter(hashtag != 'bollywood') %>% 
  with(wordcloud(hashtag, n, random.order = FALSE, max.words = 50, colors = pal))
```

```{r}
df %>% 
  arrange(-favorite_count) %>%
  head(5) %>% 
  select(favorite_count, text, favorite_count)

```
```{r}
df %>% 
  count(screen_name, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(screen_name, n)))+
  geom_col()+
  labs(title = "Top Screen Names", x = 'Frequency', y = '', caption = "The top screen name is tellychakkar, which is an online publication source that covers TV, Digital, and Bollywood News.")

```

```{r}
df %>% 
  unnest_tokens(mentions, text, "tweets", to_lower = FALSE) %>%
  filter(str_detect(mentions, "^@")) %>%  
  count(mentions, sort = TRUE) %>%
  head(10)
```
```{r}
df %>% 
  unnest_tokens(mentions, text, "tweets", to_lower = FALSE) %>%
  filter(str_detect(mentions, "^@")) %>%  
  count(mentions, sort = TRUE) %>%
  head(10) %>% 
  ggplot(aes(x = n, y = reorder(mentions, n)))+
  geom_col()+
  labs(title = "Top Mentions", x = 'Frequency', y = '', caption = "This chart shows the top mentions under the Twitter topic 'Bollywood'. Most of these mentiones correspond to famous actors and actresses, with some mentions relating to radio and media channels. One mention I found interesting is bollycoin, which I learned is Bollywood cryptocurrency.")
```

```{r}
df %>% 
  count(source, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(source, n)))+
  geom_col()+
  labs(title = "Top Sources for Twitter Users", x = 'Frequency', y = '', caption = "It seems that Android is very popular among people who tweet a lot Bollywood.")
```

```{r}
df %>% 
  filter(!is.na(country)) %>% 
  count(country, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(country, n)))+
  geom_col()+
  labs(title= "Top Countries", x = 'Frequency', y = '', caption = "It is not a suprise that India is the top country tweeting about Bollywood.")
```

```{r}
df %>% 
  filter(!is.na(location), !location=='') %>% 
  count(location, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(location, n)))+
  geom_col()+
  labs(title = "Top Locations", x = 'Frequency', y = '', caption = "The top location obviously includes India, with Mumbai and Delhi following after.")
```

```{r}
df %>% 
  select(screen_name, favourites_count) %>% 
  filter(!is.na(favourites_count)) %>% 
  group_by(screen_name) %>% 
  summarise(average_fav = mean(favourites_count)) %>% 
  arrange(-average_fav) %>% 
  head(5) %>% 
  ggplot(aes(x=average_fav, y = reorder(screen_name, average_fav)))+
  geom_col()+
  labs(title = "Top Accounts and Average Favorites Count", y='Account', caption = "The top account is RadioKC.")
```

```{r}
df %>% 
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(get_stopwords()) %>% 
  count(word, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(word, n)))+
  geom_col()+
  labs(x = 'Frequency', y = '')
```
```{r}
#filtering out unnecessary words
df %>% 
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(get_stopwords()) %>% 
  filter(!word %in% c('https', 't.co', 'amp')) %>% 
  count(word, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(word, n)))+
  geom_col()+
  labs(title = "Top Words", x = 'Frequency', y = 'Words')
```

```{r}
words <- df %>%
  mutate(text = str_remove_all(text, "&amp;|&lt;|&gt;"),
         text = str_remove_all(text, "\\s?(f|ht)(tp)(s?)(://)([^\\.]*)[\\.|/](\\S*)"),
         text = str_remove_all(text, "[^\x01-\x7F]")) %>% 
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word,
        !word %in% str_remove_all(stop_words$word, "'"),
        str_detect(word, "[a-z]"),
        !str_detect(word, "^#"),         
        !str_detect(word, "@\\S+"),
        !word %in% c('indian', 'bollywood', 'movies')) %>%
  count(word, sort = TRUE)

al <- brewer.pal(8,"Dark2")
library(wordcloud) 
words %>% 
  with(wordcloud(word, n, random.order = FALSE, max.words = 50, colors = pal))
```

```{r}
ts_plot(df, "hours") +
  labs(x = NULL, y = NULL,
       title = "Frequency of tweets",
       subtitle = paste0(format(min(df$created_at), "%d %B %Y"), " to ", format(max(df$created_at),"%d %B %Y")),
       caption = "Data collected from Twitter's REST API via rtweet") +
  theme_minimal()
```

```{r}
df %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(word, sort = TRUE) %>%
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(y= reorder(sentiment, n), x = n))+geom_col()+
    labs(title = "Sentiment Frequency", y='Relative Frequency', x ='', caption = "Most tweets have a positive sentiment.")
```
```{r}
df %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(word, sort = TRUE) %>%
    inner_join(get_sentiments("bing")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(y= reorder(sentiment, n), x = n))+geom_col()+
    labs(y='Relative Frequency', x ='')
```
```{r}
df %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(word, sort = TRUE) %>%
    inner_join(get_sentiments("afinn")) %>%
    filter(!is.na(value)) %>%
    count(value, wt = n, sort = TRUE) %>% 
    ggplot(aes(x= value, y = n))+geom_col()+
    labs(y='Frequency', x ='')
```

2. Choose a location then pick a trending keyword/hashtag in the location. Download the data associated with the keyword/hashtag. Plot at least 10 plots to visualize the data associated with the keyword/hashtag. All plots should have titles and captions. 
```{r}
# Show available locations to get trends
trends_available()
```

```{r}
get_trends('Boston')
```

```{r}
d <- read_csv('twitter_omicron.csv')
```

```{r}
#keyword_search = 'omicron'

#d <- search_tweets(q = keyword_search, 
#                        n = 18000,
#                        include_rts = FALSE,
#                        `-filter` = "replies",
#                        lang = "en") %>% mutate(created_at = #ymd_hms(format(created_at, tz = "US/Eastern")))

#write_csv(d, 'twitter_omicron.csv')
```

```{r}
d %>% 
  unnest_tokens(output = hashtag, input = text, token = "tweets") %>%
  filter(!hashtag %in% c("#covid19", "#variant", "#omicron"), str_detect(hashtag, "^#")) %>% 
  count(hashtag, sort = TRUE) %>%
  head(10) %>%
  ggplot(aes(x = n, y = reorder(hashtag, n))) +
  geom_col() +
  labs(title = "Top Hashtags", y = '', x = 'Frequency', caption = "The top hashtag is #omicron.")
```

```{r}
library(RColorBrewer)

pal <- brewer.pal(8,"Dark2")
library(wordcloud) 

d %>% 
  unnest_tokens(output = hashtag, input = text, token = "tweets") %>%
  filter(!hashtag %in% c("#omicron",'#covid19'), str_detect(hashtag, "^#")) %>%
  count(hashtag, sort = TRUE) %>%
  mutate(hashtag = str_remove(hashtag, '#')) %>% 
  filter(hashtag != 'covid') %>% 
  with(wordcloud(hashtag, n, random.order = FALSE, max.words = 50, colors = pal))
```

```{r}
d %>% 
  unnest_tokens(mentions, text, "tweets", to_lower = FALSE) %>%
  filter(str_detect(mentions, "^@")) %>%  
  count(mentions, sort = TRUE) %>%
  head(10) %>% 
  ggplot(aes(x = n, y = reorder(mentions, n)))+
  geom_col()+
  labs(x = 'Frequency', y = '', title = "Top Mentions", caption = "Most mentions are news and media channels.")
```

```{r}
d %>% 
  count(screen_name, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(screen_name, n)))+
  geom_col()+
  labs(x = 'Frequency', y = '', title = "Top Tweeters", caption = "The top tweeter is PartyOmicron.")
```

```{r}
d %>% 
  count(source, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(source, n)))+
  geom_col()+
  labs(x = 'Frequency', y = '', title = "Tweet Sources", caption = "Most tweets seem are sourced from the Twitter Web App.")
```

```{r}
d %>% 
  filter(!is.na(country)) %>% 
  count(country, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(country, n)))+
  geom_col()+
  labs(x = 'Frequency', y = '', title =  "Top Countries ", caption = " The top 3 countries include the UK, US, and India. This is most likely because the Omicron variant is supposedly spreading in these three areas the most.")
```

```{r}
d %>% 
  filter(!is.na(location), !location=='') %>% 
  count(location, sort = TRUE) %>%
  head(5) %>% 
  ggplot(aes(x = n, y = reorder(location, n)))+
  geom_col()+
  labs(x = 'Frequency', y = '', title = "Tweet Locations", caption = "Related to the countries visual, the top location is London England, with India being next.")
```

```{r}
d %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(word, sort = TRUE) %>%
    inner_join(get_sentiments("afinn")) %>%
    filter(!is.na(value)) %>%
    count(value, wt = n, sort = TRUE) %>% 
    ggplot(aes(x= value, y = n))+geom_col()+
    labs(y='Frequency', x ='', title = "Tweet Sentiment Analysis", caption = "Most of the tweets are showing a larger negative sentiment than a positive.")
```

```{r}
ts_plot(d, "hours") +
  labs(x = NULL, y = NULL,
       title = "Frequency of tweets",
       subtitle = paste0(format(min(df$created_at), "%d %B %Y"), " to ", format(max(df$created_at),"%d %B %Y")),
       caption = "Data collected from Twitter's REST API via rtweet. There is a huge spike at 12:00.") +
  theme_minimal()
```

```{r}
d %>%
    unnest_tokens(input = text, output = word) %>% 
    anti_join(get_stopwords()) %>% 
    count(word, sort = TRUE) %>%
    inner_join(get_sentiments("nrc")) %>%
    filter(!is.na(sentiment)) %>%
    count(sentiment, sort = TRUE) %>% 
    mutate(n = n/sum(n)) %>% 
    ggplot(aes(y= reorder(sentiment, n), x = n))+geom_col()+
    labs(y='Relative Frequency', x ='', title = "Sentiment Analysis by Emotion", caption = "It is not suprising that a negative sentiment has the highest relative frequency.")
```

